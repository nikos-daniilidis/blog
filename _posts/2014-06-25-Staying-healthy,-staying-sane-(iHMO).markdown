---
layout: post
title:  "Staying healthy, staying sane (iHMO)"
date:   2014-06-25 10:00:00
categories: jekyll update
---
Last Friday, I needed a doctor and was trying to get my HMO plan set up (I'm fine now). I soon found out that my insurance company was determined to stand up to the bad reputation that HMO insurance providers have. Besides poor and non-transparent over-the-phone customer service, on my provider's website only 5 out of 300 doctors had any rating at all. So, I decided to go online and find myself a decent primary care doctor. I did not feel like typing in the names of 300 doctors on doctor rating websites, so I programmed my computer to do that. What follows is an outline of the (interesting) results.

First, a word of introduction for those who are not lucky enough to live in the country with the best healthcare system in the world. Health insurance plans in this country fall under two categories. They are either HMO plans or PPO plans. Roughly speaking, with an HMO plan you have lower co-payments and healthcare costs, but you are extremely restricted on the doctors you are allowed to see. Most of your health issues have to go through a specific doctor, your 'primary care' doctor. If you get a hurting foot, you have to go through your primary care doctor, who will send you to the foot doctor. With a PPO plan you have more freedom, you can see the foot doctor directly, as long as they are within a network of allowed providers. Nationwide, 23.3% of the population holds some kind of HMO health insurance. For the state of California, the percentage is higher: 43.5% of the ppopulaion has HMO coverage ([source](http://kff.org/other/state-indicator/hmo-penetration-rate/)).

Now, HMO plans have a reputation for not trying to give you the best possible healthcare. That's why it matters to carefully choose your primary care doctor. Thankfully there are websites where you can go and find doctor ratings. The problem with these is that they are often involved in controversial cases of rigged ratings ([e.g.](http://www.eastbayexpress.com/gyrobase/yelp_and_the_business_of_extortion_2_0/Content?oid=927491&page=1)). However, a lot of people visit and rate in those websites. So, hopefully with large numbers of visitors and ratings you can have some faith in the results (or can you?) Here is representative traffic for those websites, in visitors per month:

Vitals.com             4.35 million

Healthgrades.com       4.28 million

Wellness.com           2.55 million

UCompareHealthCare.com 1.92 million

RateMds.com              732,943

([source](http://www.slideshare.net/reviewconcierge/remove-review-from-the-top-5-healthcare-directories))

Now, back to my problem: I tried to find out what ratings I can trust (and which doctor I should choose) for the list of doctors that my insurer gave me as possible primary care providers. From the health insurance website, I downloaded as pdf the list of doctors I am eligible to see, and converted the pdf to a text file. I ended up with a list of 286 doctor names and addresses. Then I wrote some python scripts which retrieved for me the average ratings of these 286 doctors from four websites: vitals.com, healthgrades.com, ucomphealthcare.com, and ratemds.com. Then I organized and plotted the ratings, as I show below.

I am sharing the code which I used to plot and analyze the doctor ratings in the linked [IPython notebook](https://github.com/nikos-daniilidis/find-md/blob/master/find_me_a_doc_nonames.ipynb) (and converted to [html](http://nikos-daniilidis.github.io/find-md/find_me_a_doc_nonames.html)). I am also sharing the dataset of doctor ratings, with the doctor names removed (on same github directory as the notebook). I am not including the part of the code which allows retrieving the ratings from the rating websites (even though it amounted to 80% of the effort to produce the results), since this code could easily be abused. If you need the application for yourself, email me and we'll see what we can do.

The following plot shows a summary of distributions of doctor ratings in the four websites, as well as the correlations between ratings of the same doctor between different websites.

![raw ratings data](/assets/2014-06-25-staying-healthy-staying-sane/scatter_matrix_94010_5mi_94709_10mi.png)

Well, what have we here? The only thing that's clear is that the data is all over the place! But first, what exactly am I plotting?

Along the diagonal, I show the histogram of ratings for each of the four websites, i.e. the histogram for healthgrade.com, the histogram for ratemds.com, the histogram for ucomphealthcare.com, and the one for vitals.com. The maximum rating for the first three is five stars, while vitals.com goes only to four stars (that's a minor point). Note that the shape of the histograms is different, i.e. different websites show different distributions (for my small sample). Three out of four seem to have inflated ratings, and only ratemds.com seems to have the bell-shaped form I would expect. Why do I expect a bell shape? Because it is more likely that some doctors will be excellent and some will be bad, but most doctors will be not particularly good or bad -falling somewhere in the middle. If all doctors in my rating pool are outstanding, I either have the problem that my doctor pool has been truncated to leave out the better ones (so that I am sampling the lower half of a bell curve), or I have a problem of 'grade inflation', which somehow follows all these doctors even now that they are out of medical school. Which of the two it is, I do not know. I will leave it up to you to speculate which of the four websites most likely suffers from erroneous ratings for doctors who refused to offer reputation management concessions...

Moving on to cross-correlations, we see that in the correlation of ratings between different websites the world falls apart! This is what you see in the curves off the diagonal, for exammple the second plot in the first row shows the ratings in ratemds.com vesrus the ratings in healthgrade.com. Each point in these off-diagonal plots represents a doctor. All points are identical and semitransparent, which means that darker blue dots reveal a high concentration of doctors in a particular region (they are multiple doctors with nearly overlapping ratings). Now, if there was good correlation for the ratings of the same doctor in different websites, I would expect the points to fall on a straight line with some scatter. That's not what I see! To quantify how much correlation there is, I try to fit a line through the data. This is the bold black line that you see in each of the curves. Around each bold line I draw, as a grey band, the 95% confidence interval for my linear fit. This band represents the range of values where 95% of my data should fall, if the linear fit is a good description of my data. Are the linear fits good descriptions? No! You can get an idea of how bad they are, by looking at the $$ R^2$$ value which I show in the title of each plot. You can take this value as giving you the fraction of variability in the data which is described by the linear model. Some pairs of websites give extremely bad results, with $$ R^2$$ as low as 0.004 for the ratings between ucomphealthcare.com and healthgrade.com. The biggest degree of linear correlation, with $$ R^2$$ around 0.25, is between ucomphealthcare.com and vitals.com.

To finish on a positive note, not all is bleak. We cannot cross calibrate the ratings, but a closer look at the plots reveals that while some low-rated doctors show a lot of scatter in their ratings , high-rated doctors tend to be highly rated in more than one website. For example, there are concentrations of dark points at the high rating regions in vitals.com, ratemds.com, and ucomphealthcare.com. This allows me to pick a good doctor with relatively high confidence. To do this, I have to pick someone who rates well in all of the available websites, say a doctor with the maximum average rating. This is not the most mathematically proper decision procedure, but it allows me to at least choose a primary care physician before I fully develop the proper decision algorithm.

I might follow up on this post with some more work on making sense out of this data. One idea to make use of all the ratings, is to do what is called principal components analysis (PCA) on the data. Principal components analysis is a standard trick for situations where you have (possibly noisy) data in a high-dimensional space, and you want to find those combinations of dimensions which contain most of the information which is hidden in your data. Then you can keep a few parameters (which are linear combinations of all dimensions) which contain most of the information in your data, and not worry about the rest. In other words PCA would allow me to generate a "hypergrade", one single parameter which tells me who is the best doctor, givem the data at hand. The reason I am not doing this right now is that PCA will not work well with situations where some data is missing, and Python does not provide an implementation of the methods which are used to solve this problem. I might return to this in about a week's time, unless I get caught up with some of my other projects in javascript, R, or Julia.

To convert the pdf to text for this work I used [PDFMiner](http://www.unixuser.org/~euske/python/pdfminer/), an awesome Python package. To get the data from the rating websites, I relied heavily on Python [requests](http://docs.python-requests.org/en/latest/), and to parse the information from the data I made heavy use of the [re](https://docs.python.org/2/library/re.html) module and Python [pattern](http://www.clips.ua.ac.be/pattern). Finally, the part of the code that you see here uses [pandas](http://pandas.pydata.org/), [numpy](http://www.numpy.org/), and [matplotlib](http://matplotlib.org/). [Same post on wordpress](http://oligotropos.wordpress.com/2014/06/25/staying-healthy-staying-sane/).
