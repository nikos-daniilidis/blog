<!DOCTYPE html>

<script type="text/javascript"
	src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Clustering of text: It is not flat!</title>
    <meta name="description" content="A blog about science, technology, society, and everything in-between.
">

    <link rel="stylesheet" href="/css/main.css">
    <link rel="canonical" href="http://yourdomain.com/jekyll/update/2014/06/15/Clustering-of-text:-It-is-not-flat!.html">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Nikos Daniilidis</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
          <a class="page-link" href="/publications/index.html">Publications</a>
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Clustering of text: It is not flat!</h1>
    <p class="post-meta">Jun 15, 2014</p>
  </header>

  <article class="post-content">
    <p>This is going to be a technical post. I am looking into the behaviour of clustering analysis for text entries. The reason is that I tried putting in clusters the title/authors-list data for the Physical Review archive and was unpleasantly surprised. I went in with high expectations: The titles in science publications have lots of condensed meaning to them, and the authors list provides one additional hint to nail down which category an article belongs to. To my surprise, I found that a simple clustering algorithm (k-means) was putting 95% of my data in the same cluster.</p>

<p>Naturally, to me this result was the equivalent of what the picture below must have been to the members of the <a href="http://theflatearthsociety.org/cms/">flat earth society</a>: I thought “Nonsense! There must be a work around this!”. Below are the efforts. Thankfully in blogging, unlike in academia, you are allowed to publish negative results ;-)</p>

<p><img src="/assets/2014-06-15-text-k-means/first-ever-earth-from-space.jpg" alt="first ever picture of earth from space" /></p>

<p><em>First ever photograph of earth from space (credits: NASA)</em></p>

<p>So, here is a statement of the problem: we have a bunch of text entries (a “corpus”) which belong to a number of different categories, say 5000 text entries from 10 categories. How can we tell a computer to sort them out? One way is declare that entries containing the same terms belong to the same cluster. In order to allow our machine to decide that, we create a collection of all the terms, numbered from 1 to N (yes, I still count form 1!). Now for each text entry, we count how  many times each term appears, and in this way convert the entry to a vector in the N-dimensional space of all terms appearing in the text corpus. Once we have done that, we can decide how much two text entries are related, based on their ‘distance’ in this N-dimensional space. Thus, we can use our favourite distance metric and clustering algorithm to split the text entries into categories. Simple, right?</p>

<p>There are some caveats about getting rid of very common and very uncommon words and about keeping the stems of words (e.g. “spher” for “sphere” and for “spherical”) which I will not go into here. All these details can be taken care of using standard tools and they result in increased signal to noise. Sadly, after all this is said and done, two pretty hairy issues remain:</p>

<p>The first has to do with the dimensionality of the vector space in which we need to cluster. The space is huge (for a text corpus of 5000 paragraph-long samples we can easily end up with 5000 dimensions i.e. 5000 distinct terms). Now each vector among our samples has much lower dimension, probably 10-40 terms, let’s say it typically extends to 20 dimensions. Clearly, we are severely undersampling the vector space. For example the probability that two 20-dimensional vectors randomly picked from a 5000 dimensional space will have any dimensions in common is around 8% (thanks to Shirley Yap for solving this problem in 3 minutes and sparing me 2 hours of adding factorials!).</p>

<p>The other issue has to do with the shape of the clusters corresponding to each category: They will most likely not be convex. In fact I am absolutely certain I will never be able to understand all the ways in which 5000-dimensional clusters can be oddly shaped.</p>

<p>The simplest clustering algorithm there is (k-means) can offer some insights. k-means is extremely simple. It tries to categorize sample vectors into k clusters, with k fixed. It starts by randomly selecting k cluster centers (or “centroids”), and places each data point in one cluster, based on which centroid is the closest to it. Then it recalculates the cluster centers as the centers of mass of the newly formed clusters, and keeps repeating the process of assigning samples to clusters and repositioning the centers until the center positions converge.</p>

<p>Instead of working with the Physical Review data, where things failed badly, let’s do the clustering analysis on a dataset where the text entries are slightly longer. We chose the “20 newsgroups-18828” data set (<a href="http://mlcomp.org/datasets">from MLcomp.org</a>), and carry out the clustering analysis on the entries found in 7 categories of this data set. To categorize the data into clusters, we run k-means with 7 clusters, and assign to each of the clusters the label of the majority of its members. The result is that an unimpressive 45% of the data end up being categorized with the wrong label. This is much better that the 86% we would expect with random group assignments, but we were hoping for more.</p>

<p>The following figure can give us an idea why: In this plot we see the probability that a randomly chosen text entry will fall into a given cluster. Darker blue means higher probability. Some of the clusters are well separated from all but one groups, for example “sci.space” corresponds pretty nicely to cluster #1. But we also see that some clusters, most notably #2 get a sizeable fraction of entries from all categories. We also see that most of the confusion is around computer hardware and graphics issues.</p>

<p><img src="/assets/2014-06-15-text-k-means/clustering-7x7.png" alt="confusion matrix for text clustering" /></p>

<p><em>Probability of assigning members of a given group to one of the clusters.</em></p>

<p>Is there a way around this? Yes, partially. We can ask k-means to use more clusters, and hope that it will be able to capture more of the fine details in the topics we are trying to differentiate. Let’s see how this goes. We push the number of clusters up to 1000, and assign each cluster to one of the seven groups according to the category of the majority of its members. We then look a the fraction of text entries which do not get properly classified in this process. The result is in the following figure (error-bars represent the standard deviation in classification error in 10 runs of the algorithm).</p>

<p><img src="/assets/2014-06-15-text-k-means/error-vs-clusters-l2-tf-idf-no-label.png" alt="error vs number of clusters" /></p>

<p><em>Classification error vs number of clusters</em></p>

<p>What an interesting figure! We see a moderate improvement in the accuracy of the classification, but at the expense of an immense increase in the number of clusters. How far can we push this approach? The extreme limit is to make the number of clusters equal to the number of text entries, around 5000 for our current example. In this limit, where on average we will have one cluster per text entry, does it make sense to talk about clustering any more? In a way, it does. The increase in number of clusters is reminiscent of what is called hierarchical clustering, where small clusters are joined into larger ones to form a hierarchy. This is essentially what we do here, where we start with many clusters and join them to form a few groups. Word on the street (i.e. on <a href="http://stats.stackexchange.com/">http://stats.stackexchange.com/</a>) has it that hierarchical clustering is preferable  to k-means for text data, which agrees with the pattern we see here.</p>

<p>So increasing the number of clusters makes sense from an intuitive ‘hierarchy of content’ point of view. There is also a geometric way to understand why more clusters help with k-means. By its construction, k-means works with convex clusters, those blobs in the feature space which have no protrusions, no indentations and no cavities. However, the clusters corresponding to the groups in our data do not necessarily have this property. Thus, by allowing for a large number of clusters which we then join together, we can conform to non-convex shapes. Interestingly, although we are using up to 1000 clusters we do not seem to have the problem of “overfitting”, i.e. making a prediction which follows every random detail in our data set, but lacks predictive power: To ensure this, we estimated the error in the above plot using a different set of data than the set which we used to label the clusters.</p>

<p>This geometric view might have something to do with an interesting aspect of the curve above. You might have noticed that the improvement in error with number of clusters is initially slow, but seems to change abruptly at around 500 clusters, where there is a kink in the curve. Below the kink, the  error drops with number of clusters as <script type="math/tex"> err \sim n_{cl}^{-0.05}</script>, while above it it drops as <script type="math/tex"> err \sim n_{cl}^{-0.35}</script>. On seeing this, the natural instinct of someone with my training is to shout “Phase Transition!”. But, perhaps, a more valid question to ask is whether this change in behavior has to do with the convexity properties of the shapes we are trying  to fit in 5000-dimensional space. I can’t promise I’ll do this, but well, hey, if you’re interested the code is available for you to build on and answer the question! To sum it all up, geometry matters. We can go back to our proverbial ‘Flat earth society’ member, and proclaim: It is flat! (depending on your coordinate system).</p>

<p>This exercise was based on the python <a href="http://scikit-learn.org/stable/">scikit-learn</a> package. As usual, you can find the code for this post on github as <a href="https://github.com/nikos-daniilidis/haystack/blob/master/clustering-performance-short.ipynb">IPython notebook</a> and converted to <a href="http://nikos-daniilidis.github.io/haystack/clustering-performance-short.html">html</a>. <a href="http://oligotropos.wordpress.com/2014/06/15/clustering-of-text-it-is-not-flat/">Same post on wordpress</a>.</p>


  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Nikos Daniilidis</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li><a href="mailto:nikos.daniilidis@gmail.com">nikos.daniilidis@gmail.com</a></li>
          
          <li><a href="https://www.linkedin.com/in/ndaniilidis">https://www.linkedin.com/in/ndaniilidis</a></li>
          
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/nikos-daniilidis">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>

              <span class="username">nikos-daniilidis</span>
            </a>
          </li>
          

          
          <li>
            <a href="https://twitter.com/ndaniilidis">
              <span class="icon  icon--twitter">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                  c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
                </svg>
              </span>

              <span class="username">ndaniilidis</span>
            </a>
          </li>
          
        </ul>
      </div>

      <div class="footer-col  footer-col-3">
        <p class="text">A blog about science, technology, society, and everything in-between.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
